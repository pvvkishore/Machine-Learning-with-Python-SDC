{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f60441f6",
   "metadata": {},
   "source": [
    "# Single Neuron (Logistic Regression) learns the AND gate — **SGD from scratch (NumPy only)**\n",
    "\n",
    "This notebook teaches how a **neural network classifier** (a single neuron) works using:\n",
    "- **logits** (pre-activation), **activation**, **probabilities**\n",
    "- **binary cross-entropy** loss\n",
    "- **backprop gradients** and **SGD updates**\n",
    "- success + failure cases, and small tasks to explore hyperparameters\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08f654",
   "metadata": {},
   "source": [
    "## 1) Theory & terminology (with formulas)\n",
    "\n",
    "### Model (1 neuron)\n",
    "**Inputs / features**:  \\(\\mathbf{x}=[x_1,x_2]^T\\)  \n",
    "**Parameters**: weights \\(\\mathbf{w}=[w_1,w_2]^T\\), bias \\(b\\)\n",
    "\n",
    "### Logit (a.k.a. score, pre-activation)\n",
    "\\[\n",
    "z = \\mathbf{w}^T\\mathbf{x} + b = w_1x_1+w_2x_2+b\n",
    "\\]\n",
    "- \\(z\\) is called the **logit** (raw score before activation).\n",
    "\n",
    "### Activation (sigmoid → probability)\n",
    "\\[\n",
    "\\hat{y}=\\sigma(z)=\\frac{1}{1+e^{-z}}\n",
    "\\]\n",
    "- \\(\\hat y \\in (0,1)\\) can be interpreted as \\(P(y=1 \\mid \\mathbf{x})\\).\n",
    "\n",
    "### Decision rule (class prediction)\n",
    "\\[\n",
    "\\hat{c}=\\mathbb{1}[\\hat{y}\\ge 0.5] \\quad\\Longleftrightarrow\\quad \\mathbb{1}[z\\ge 0]\n",
    "\\]\n",
    "\n",
    "### Loss (Binary Cross-Entropy, BCE)\n",
    "\\[\n",
    "L(\\hat y,y)= -\\left(y\\ln(\\hat y) + (1-y)\\ln(1-\\hat y)\\right)\n",
    "\\]\n",
    "\n",
    "### Backprop (sigmoid + BCE gives a simple gradient)\n",
    "\\[\n",
    "\\delta=\\frac{\\partial L}{\\partial z} = \\hat y - y\n",
    "\\]\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial w_i}=\\delta x_i,\\quad \\frac{\\partial L}{\\partial b}=\\delta\n",
    "\\]\n",
    "\n",
    "### SGD update (learning rate \\(\\eta\\))\n",
    "\\[\n",
    "w_i \\leftarrow w_i-\\eta(\\hat y-y)x_i,\\quad b\\leftarrow b-\\eta(\\hat y-y)\n",
    "\\]\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427525",
   "metadata": {},
   "source": [
    "## 2) Student tasks (do these by editing a few numbers)\n",
    "\n",
    "1. **Learning rate sweep**: change `lr` to `0.001`, `0.01`, `0.1`, `1.0`, `5.0` and re-run training.  \n",
    "   - What happens to the loss curve? Does it converge or blow up?\n",
    "2. **Epochs**: increase/decrease `epochs` (e.g., 20 vs 500).  \n",
    "   - How many epochs are needed for perfect AND accuracy?\n",
    "3. **Initialization**: try starting with `w=[0,0]`, `w=[-1,-1]`, or random.  \n",
    "   - Does learning still work?\n",
    "4. **Threshold**: change the decision threshold from `0.5` to `0.7` or `0.3`.  \n",
    "   - How does accuracy change?\n",
    "5. **Failure case**: run the **no-bias** experiment and explain why AND cannot be learned without bias.\n",
    "6. **Shuffle vs fixed order**: change the RNG seed.  \n",
    "   - Does the loss curve look different?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6301919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "print('NumPy version:', np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f08a4",
   "metadata": {},
   "source": [
    "## 3) AND dataset (features + labels)\n",
    "\n",
    "AND truth table:\n",
    "- (0,0) → 0\n",
    "- (0,1) → 0\n",
    "- (1,0) → 0\n",
    "- (1,1) → 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4401140b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=\n",
      " [[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]] \n",
      " y= [0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "X=np.array([[0.,0.],[0.,1.],[1.,0.],[1.,1.]]) # Input \n",
    "y=np.array([0.,0.,0.,1.]) # Target Classes - class 0 - 0, class 1 - 1\n",
    "print('X=\\n',X,'\\n y=',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a6ef7",
   "metadata": {},
   "source": [
    "## 4) Train/test split (tiny but valid)\n",
    "\n",
    "We’ll train on 3 samples (including the positive one) and test on 1 sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a62966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X:\n",
      " [[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 1.]] \n",
      "Train y: [0. 0. 1.] \n",
      "Test X:\n",
      " [[1. 0.]] \n",
      "Test y: [0.]\n"
     ]
    }
   ],
   "source": [
    "# Standard train - 70% data and test - 30%\n",
    "train_idx=np.array([0,1,3])\n",
    "test_idx=np.array([2])\n",
    "Xtr,ytr=X[train_idx],y[train_idx] # Train data [00,01,11]-[0,0,1]\n",
    "Xte,yte=X[test_idx],y[test_idx] # Test data [10] - [0]\n",
    "print('Train X:\\n',Xtr,'\\nTrain y:',ytr,'\\nTest X:\\n',Xte,'\\nTest y:',yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc744da6",
   "metadata": {},
   "source": [
    "## 5) Activation and loss (NumPy only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892211a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([-1,0,1]) = [0.000045 0.5      0.999955]\n",
      "sigmoid(0) = 0.5\n"
     ]
    }
   ],
   "source": [
    "# Y_pred = W*x+b -Linear - Logits\n",
    "# Converts raw neuron outputs to probability scores\n",
    "def sigmoid(z): return 1/(1+np.exp(-z)) # Probability score sig(Y_pred)\n",
    "print('sigmoid([-1,0,1]) =', sigmoid(np.array([-10.,0.,10.])))\n",
    "print('sigmoid(0) =', sigmoid(0.0))\n",
    "# Loss  = y - y_pred**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c119727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bce(yhat=0.5,y=1) = 0.6931471805599453\n",
      "bce(yhat=0.5,y=0) = 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "# Binary Cross entropy loss Y_hat - Y_pred\n",
    "def bce(yhat,y,eps=1e-12):\n",
    "    yhat=np.clip(yhat,eps,1-eps)\n",
    "    return -(y*np.log(yhat)+(1-y)*np.log(1-yhat))\n",
    "print('bce(yhat=0.5,y=1) =', float(bce(0.5,1.0))) # replace 0.5 by 0.75\n",
    "print('bce(yhat=0.5,y=0) =', float(bce(0.5,0.0))) # 0.5 by 0.25\n",
    "# bec_loss = -(y_target*log(y_pred)+(1-y_target)*log(1-y_pred))\n",
    "# Categorical cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c23c79",
   "metadata": {},
   "source": [
    "## 6) Initialize parameters (given)\n",
    "\n",
    "We start from the user-provided values:\n",
    "\\[\n",
    "w_1=5,\\; w_2=5,\\; b=-15\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3712ef48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init w= [5. 5.]  b= -15.0  lr= 0.1  epochs= 50\n"
     ]
    }
   ],
   "source": [
    "# No. of inputs - 2 - 2 weights\n",
    "w=np.array([5.,5.]); b=-15.0\n",
    "lr=0.1; epochs=50\n",
    "print('init w=',w,' b=',b,' lr=',lr,' epochs=',epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1c6ab",
   "metadata": {},
   "source": [
    "## 7) Forward pass (initial): logits and activations on full dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c227a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits z = [-15. -10. -10.  -5.] \n",
      "probs yhat = [0.       0.000045 0.000045 0.006693]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=X@w+b # Learning model z - logits\n",
    "yhat=sigmoid(z) # Prob scores\n",
    "print('logits z =', z,'\\nprobs yhat =', yhat)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81a09daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss per sample = [0.       0.000045 0.000045 5.006715]\n",
      "mean loss = 1.2517016130474563\n"
     ]
    }
   ],
   "source": [
    "L=bce(yhat,y)\n",
    "print('loss per sample =', L)\n",
    "print('mean loss =', float(L.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0761e",
   "metadata": {},
   "source": [
    "## 8) Train with SGD (one update per sampled training point)\n",
    "\n",
    "We record:\n",
    "- `loss_steps`: loss after **each SGD step**\n",
    "- `epoch_loss`: mean loss per epoch (each epoch has `len(ytr)` steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c63a196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained w= [7.423593 7.41127 ]  b= -12.58873755162908  final epoch loss= 0.0363193467700361\n"
     ]
    }
   ],
   "source": [
    "loss_steps=[] # Loss pre step\n",
    "rng=np.random.default_rng(0) # Random number gen\n",
    "steps=epochs*len(ytr)\n",
    "for t in range(steps):\n",
    "    i=rng.integers(len(ytr))\n",
    "    z=Xtr[i]@w+b\n",
    "    yh=sigmoid(z)\n",
    "    d=yh-ytr[i]\n",
    "    w-=lr*d*Xtr[i] # w_new = w_old - lr*dl/dw\n",
    "    b-=lr*d # b_new = b_old - lr*dl/db\n",
    "    loss_steps.append(float(bce(yh,ytr[i])))\n",
    "epoch_loss=np.mean(np.array(loss_steps).reshape(epochs,len(ytr)),axis=1)\n",
    "print('trained w=',w,' b=',b,' final epoch loss=',float(epoch_loss[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2495a8",
   "metadata": {},
   "source": [
    "## 9) Evaluate: train set predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d92545c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train probs = [0.000003 0.005611 0.904316] \n",
      "train pred = [0 0 1]\n",
      "train true = [0 0 1]\n"
     ]
    }
   ],
   "source": [
    "ztr=Xtr@w+b; ytr_hat=sigmoid(ztr); ptr=(ytr_hat>=0.5).astype(int)\n",
    "print('train probs =', ytr_hat,'\\ntrain pred =', ptr)\n",
    "print('train true =', ytr.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a7e87",
   "metadata": {},
   "source": [
    "## 10) Evaluate: test set predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b37ea433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test probs = [0.00568] \n",
      "test pred = [0]\n",
      "test true = [0]\n"
     ]
    }
   ],
   "source": [
    "zte=Xte@w+b; yte_hat=sigmoid(zte); pte=(yte_hat>=0.5).astype(int)\n",
    "print('test probs =', yte_hat,'\\ntest pred =', pte)\n",
    "print('test true =', yte.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c245654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy train = 1.0  accuracy test = 1.0\n",
      "final parameters: w= [7.423593 7.41127 ]  b= -12.58873755162908\n"
     ]
    }
   ],
   "source": [
    "acc_tr=float((ptr==ytr).mean()); acc_te=float((pte==yte).mean())\n",
    "print('accuracy train =', acc_tr,' accuracy test =', acc_te)\n",
    "print('final parameters: w=', w,' b=', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d9feb",
   "metadata": {},
   "source": [
    "## 11) “Visualization” of loss (ASCII sparkline)\n",
    "\n",
    "Because we restrict ourselves to **NumPy only**, we draw the loss curve as a text sparkline.\n",
    "- Lower is better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80a55257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss sparkline (down is better):\n",
      " ▇▁▁▁▁▇▇▁▆▁▁▁▅▁▁▄▃▁▃▁▁▁▁▁▃▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n"
     ]
    }
   ],
   "source": [
    "a=np.array(loss_steps); chars=np.array(list('▁▂▃▄▅▆▇█'))\n",
    "s=''.join(chars[((a-a.min())/(a.ptp()+1e-12)*(len(chars)-1)).astype(int)][np.linspace(0,len(a)-1,80).astype(int)])\n",
    "print('loss sparkline (down is better):\\n', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58169f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss first 10 = [1.668942 0.       0.       3.042674 1.374954 1.278442 1.183033 2.086221\n",
      " 0.907795 1.559814]\n",
      "epoch_loss last  10 = [0.050207 0.045534 0.047116 0.083591 0.078286 0.039128 0.003558 0.07349\n",
      " 0.005536 0.036319]\n",
      "min epoch loss = 3.3784431164144464e-07\n"
     ]
    }
   ],
   "source": [
    "print('epoch_loss first 10 =', epoch_loss[:10])\n",
    "print('epoch_loss last  10 =', epoch_loss[-10:])\n",
    "print('min epoch loss =', float(epoch_loss.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e3242",
   "metadata": {},
   "source": [
    "## 12) Full truth table after training (logits, probabilities, predicted class)\n",
    "Columns: \\(x_1, x_2, y, z, \\hat y, \\hat c\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "434a98a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: x1 x2 y z yhat pred\n",
      " [[  0.         0.         0.       -12.588738   0.000003   0.      ]\n",
      " [  0.         1.         0.        -5.177467   0.005611   0.      ]\n",
      " [  1.         0.         0.        -5.165145   0.00568    0.      ]\n",
      " [  1.         1.         1.         2.246125   0.904316   1.      ]]\n"
     ]
    }
   ],
   "source": [
    "z_all=X@w+b; y_all=sigmoid(z_all); p_all=(y_all>=0.5).astype(int)\n",
    "table=np.c_[X,y,z_all,y_all,p_all]\n",
    "print('cols: x1 x2 y z yhat pred\\n', table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c97ea05",
   "metadata": {},
   "source": [
    "# 13) Failure case: **remove the bias** (b fixed to 0)\n",
    "\n",
    "A key lesson: **AND cannot be learned without a bias term**.  \n",
    "Reason: without \\(b\\), the decision boundary must pass through the origin, and you cannot separate only \\((1,1)\\) as positive while keeping \\((1,0)\\) and \\((0,1)\\) negative.\n",
    "\n",
    "We will train **only weights** and keep \\(b=0\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "467ab1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no-bias init w= [0. 0.]  b= 0.0  lr= 0.1  epochs= 200\n"
     ]
    }
   ],
   "source": [
    "w_nb=np.array([0.,0.]); b_nb=0.0\n",
    "lr_nb=0.1; epochs_nb=200\n",
    "print('no-bias init w=',w_nb,' b=',b_nb,' lr=',lr_nb,' epochs=',epochs_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fdd3e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained (no bias) w= [ 4.005458 -1.697201]  final epoch loss= 0.12080642879382007\n"
     ]
    }
   ],
   "source": [
    "loss_nb=[]; rng=np.random.default_rng(0); steps=epochs_nb*len(ytr)\n",
    "for t in range(steps): i=rng.integers(len(ytr)); z=Xtr[i]@w_nb+b_nb; yh=sigmoid(z); d=yh-ytr[i]; w_nb-=lr_nb*d*Xtr[i]; loss_nb.append(float(bce(yh,ytr[i])))\n",
    "epoch_nb=np.mean(np.array(loss_nb).reshape(epochs_nb,len(ytr)),axis=1); print('trained (no bias) w=',w_nb,' final epoch loss=',float(epoch_nb[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f432ad12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no-bias probs = [0.5      0.154831 0.98211  0.909559] \n",
      "no-bias pred = [1 0 1 1]\n",
      "true y = [0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "z_fail=X@w_nb+b_nb; y_fail=sigmoid(z_fail); p_fail=(y_fail>=0.5).astype(int)\n",
    "print('no-bias probs =', y_fail,'\\nno-bias pred =', p_fail)\n",
    "print('true y =', y.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ac605d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no-bias accuracy on all 4 = 0.5\n",
      "why it fails: without bias, (1,0) and (0,1) cannot both be <0 while (1,1) >0\n"
     ]
    }
   ],
   "source": [
    "acc_fail=float((p_fail==y).mean())\n",
    "print('no-bias accuracy on all 4 =', acc_fail)\n",
    "print('why it fails: without bias, (1,0) and (0,1) cannot both be <0 while (1,1) >0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf79fc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no-bias loss sparkline:\n",
      " ▇▇▆▆▅▇▅▅▄▃▃▇▇▅▇▅▅▂▇▅▄▄▇▂▂▁▁▁▄▁▇▇▇▇▇▁▃▇▁▁▃▁▃▃▇▂▇▂▇▇▂▇▂▇▂▂▁▁▁▇▇▇▇▂▁▁▇▇▇▇▁▇▇▁▂▇▂▇▁▁\n"
     ]
    }
   ],
   "source": [
    "a=np.array(loss_nb); chars=np.array(list('▁▂▃▄▅▆▇█'))\n",
    "s=''.join(chars[((a-a.min())/(a.ptp()+1e-12)*(len(chars)-1)).astype(int)][np.linspace(0,len(a)-1,80).astype(int)])\n",
    "print('no-bias loss sparkline:\\n', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3800c9a3",
   "metadata": {},
   "source": [
    "# 14) Failure case: learning rate too large (unstable / oscillating)\n",
    "\n",
    "If `lr` is too big, SGD updates can overshoot and make learning unstable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9c0e9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad-lr init w= [5. 5.]  b= -15.0  lr= 20.0  steps= 60\n"
     ]
    }
   ],
   "source": [
    "w_bad=np.array([5.,5.]); b_bad=-15.0\n",
    "lr_bad=20.0; steps_bad=60; rng=np.random.default_rng(1)\n",
    "print('bad-lr init w=',w_bad,' b=',b_bad,' lr=',lr_bad,' steps=',steps_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63de0a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after bad-lr steps: w= [44.762242  4.762705]  b= -35.08423476404552  finite? True\n"
     ]
    }
   ],
   "source": [
    "loss_bad=[]; np.seterr(over='ignore', under='ignore')\n",
    "for t in range(steps_bad): i=rng.integers(len(ytr)); z=Xtr[i]@w_bad+b_bad; yh=sigmoid(z); d=yh-ytr[i]; w_bad-=lr_bad*d*Xtr[i]; b_bad-=lr_bad*d; loss_bad.append(float(bce(yh,ytr[i])))\n",
    "print('after bad-lr steps: w=',w_bad,' b=',b_bad,' finite?', bool(np.isfinite(w_bad).all() and np.isfinite(b_bad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86072c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad-lr loss sparkline:\n",
      " ▁▁▄▁▄▁▁▁▁▁▁▇▁▄▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ \n",
      "last loss = 5.351708045459909e-07\n"
     ]
    }
   ],
   "source": [
    "a=np.array(loss_bad); chars=np.array(list('▁▂▃▄▅▆▇█'))\n",
    "s=''.join(chars[((a-a.min())/(a.ptp()+1e-12)*(len(chars)-1)).astype(int)][np.linspace(0,len(a)-1,60).astype(int)])\n",
    "print('bad-lr loss sparkline:\\n', s,'\\nlast loss =', loss_bad[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e0558b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad-lr probs = [0.       0.       0.999937 0.999999] \n",
      "bad-lr pred = [0 0 1 1]\n",
      "true y = [0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "z_bad=X@w_bad+b_bad; y_bad=sigmoid(z_bad); p_bad=(y_bad>=0.5).astype(int)\n",
    "print('bad-lr probs =', y_bad,'\\nbad-lr pred =', p_bad)\n",
    "print('true y =', y.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b07ee2",
   "metadata": {},
   "source": [
    "## What you should take away\n",
    "\n",
    "- **Logit** \\(z=w^Tx+b\\) is the raw score; **sigmoid activation** turns it into a probability.\n",
    "- With BCE + sigmoid, the backprop signal is simply \\(\\delta=\\hat y-y\\).\n",
    "- **Bias is essential** for AND (without it, you cannot isolate only \\((1,1)\\) as positive).\n",
    "- Hyperparameters like learning rate and epochs strongly affect convergence.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
